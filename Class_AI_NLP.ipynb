{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing the Libraries"
      ],
      "metadata": {
        "id": "JvU7AYqNiNLv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51EER3NAVHSm"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "import gensim\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams\n",
        "from sklearn import preprocessing #scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##LOWERCASE Pre-processing\n",
        "\n",
        "text_data = \"\"\"\n",
        "Let's convert this demo text to Lowercase for this NLP Tutorial using NLTK. NLTK stands for Natural Language Toolkit\n",
        "\"\"\"\n",
        "lower_text = text_data.lower()\n",
        "print (lower_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gK6iRo3VpcH",
        "outputId": "7a434a97-0448-49c7-89d3-2c89d8d0e0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "let's convert this demo text to lowercase for this nlp tutorial using nltk. nltk stands for natural language toolkit\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzocVEXcWMnG",
        "outputId": "c4c438c9-f865-4d23-dea9-473003a948c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens = word_tokenize(text_data)\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVu75w92Vq3n",
        "outputId": "eee980a3-0c85-4e95-ca31-08a9d0e3b004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let', \"'s\", 'convert', 'this', 'demo', 'text', 'to', 'Lowercase', 'for', 'this', 'NLP', 'Tutorial', 'using', 'NLTK', '.', 'NLTK', 'stands', 'for', 'Natural', 'Language', 'Toolkit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1v1A6UAWor8",
        "outputId": "cf15ccaf-9a2c-4584-83be-409b64470fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopword = stopwords.words('english')\n",
        "removing_stopwords = [word for word in word_tokens if word not in stopword]\n",
        "print (removing_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcwzhKZBWTTg",
        "outputId": "15a8e84c-cd5e-4f5d-e48b-5f5d47f629ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let', \"'s\", 'convert', 'demo', 'text', 'Lowercase', 'NLP', 'Tutorial', 'using', 'NLTK', '.', 'NLTK', 'stands', 'Natural', 'Language', 'Toolkit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer() #most commonly used stemmers\n",
        "stemmed_words = [ps.stem(word) for word in word_tokens]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvqbhwwkW1o2",
        "outputId": "8cffadd0-9d3d-4ce9-f5f6-99c0a76a4238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['let', \"'s\", 'convert', 'thi', 'demo', 'text', 'to', 'lowercas', 'for', 'thi', 'nlp', 'tutori', 'use', 'nltk', '.', 'nltk', 'stand', 'for', 'natur', 'languag', 'toolkit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-sBKOhKXMjf",
        "outputId": "519d2a0f-b833-4de1-c503-1692b7d5d52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "word_tokens2 = [\"corpora\",\"better\",\"rocks\",\"caring\",\"classes\"]\n",
        "lemmatized_word = [wnl.lemmatize(word)\n",
        "for word in word_tokens2]\n",
        "print (lemmatized_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfV6Q81bXB-Z",
        "outputId": "51a158ea-1ce6-43c0-f5a5-d1c4a44b599b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['corpus', 'better', 'rock', 'caring', 'class']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's convert this demo text to Lowercase for this NLP Tutorial using NLTK. NLTK stands for Natural Language Toolkit"
      ],
      "metadata": {
        "id": "thRsNcDQEmYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_grams = ngrams(text_data.split(), 3)\n",
        "for grams in n_grams:\n",
        "  print(grams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIJKplLIXSns",
        "outputId": "4e287826-0010-4a08-8bcd-44789ddac960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"Let's\", 'convert', 'this')\n",
            "('convert', 'this', 'demo')\n",
            "('this', 'demo', 'text')\n",
            "('demo', 'text', 'to')\n",
            "('text', 'to', 'Lowercase')\n",
            "('to', 'Lowercase', 'for')\n",
            "('Lowercase', 'for', 'this')\n",
            "('for', 'this', 'NLP')\n",
            "('this', 'NLP', 'Tutorial')\n",
            "('NLP', 'Tutorial', 'using')\n",
            "('Tutorial', 'using', 'NLTK.')\n",
            "('using', 'NLTK.', 'NLTK')\n",
            "('NLTK.', 'NLTK', 'stands')\n",
            "('NLTK', 'stands', 'for')\n",
            "('stands', 'for', 'Natural')\n",
            "('for', 'Natural', 'Language')\n",
            "('Natural', 'Language', 'Toolkit')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word Vectorization**"
      ],
      "metadata": {
        "id": "NYm0fkowqgNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **One Hot Vector Encoding**"
      ],
      "metadata": {
        "id": "5sHey3lUrmmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens3 = ['corpora', 'better', 'rocks', 'care', 'classes','better','apple']\n",
        "lab_encoder = preprocessing.LabelEncoder()\n",
        "int_label_encoder = lab_encoder.fit_transform(word_tokens3)\n",
        "lab_encoded = int_label_encoder.reshape(len(int_label_encoder),1)\n",
        "one_hot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "one_hot_encoded = one_hot_encoder.fit_transform(lab_encoded)\n",
        "print(one_hot_encoded)\n",
        "print(word_tokens3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aGKD6imX6Sc",
        "outputId": "8ffe8a3b-6a33-42e2-e024-636bff456816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]]\n",
            "['corpora', 'better', 'rocks', 'care', 'classes', 'better', 'apple']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NAMED ENTITY RECOGNITION"
      ],
      "metadata": {
        "id": "bsFM_moeE8m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Process whole documents\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "\"Google in 2007, few people outside of the company took him \"\n",
        "\"seriously. “I can tell you very senior CEOs of major American \"\n",
        "\"car companies would shake my hand and turn away because I wasn’t \"\n",
        "\"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "\"this week.\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "for entity in doc.ents:\n",
        "  print(entity.text, entity.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ffqQiHuaMKR",
        "outputId": "007641b6-c2a9-43c9-b411-08b466c5e7f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sebastian Thrun PERSON\n",
            "Google ORG\n",
            "2007 DATE\n",
            "American NORP\n",
            "Thrun PERSON\n",
            "Recode ORG\n",
            "earlier this week DATE\n"
          ]
        }
      ]
    }
  ]
}